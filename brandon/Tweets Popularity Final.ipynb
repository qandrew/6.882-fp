{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import scipy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xlrd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../RetweetDataAOAS/retweet_data/'\n",
    "root_tweet_names = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "num_root_tweets = len(root_tweet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_name_to_index = {}\n",
    "for i in range(num_root_tweets):\n",
    "    tweet_name_to_index[root_tweet_names[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_partition_file_name(name):\n",
    "    root = name.split('.')\n",
    "    items = root[0].split('_')\n",
    "    items[-2], items[-1] = items[-1], items[-2]\n",
    "    return \".\".join([\"_\".join(items), root[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_path = '../Partition_1.xlsx'\n",
    "partition = pd.read_excel(partition_path)\n",
    "partition_assignment = {}\n",
    "for index, row in partition.iterrows():\n",
    "    training_file_name = format_partition_file_name(row['Training'])\n",
    "    prediction_file_name = format_partition_file_name(row['Prediction'])\n",
    "    partition_assignment[tweet_name_to_index[training_file_name]] = True\n",
    "    partition_assignment[tweet_name_to_index[prediction_file_name]] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces a dictionary of dataframes for each tweetfile, with initial \n",
    "# preprocessing\n",
    "observation_probability = .25\n",
    "fields = ['RetweetCount', 'UserId', 'ScreenName', 'FollowerCount', \n",
    "          'DistanceFromRoot','Time', 'ParentScreenName', 'Text']\n",
    "tweet_dfs = []\n",
    "for i in range(num_root_tweets):\n",
    "    tweet_df = pd.read_csv(path+root_tweet_names[i], sep=\"\\t\", header=None, \n",
    "                         quoting=csv.QUOTE_NONE, names=fields, encoding = \"ISO-8859-1\")\n",
    "    if not partition_assignment[i]:\n",
    "        tweet_df = tweet_df.head(int(observation_probability * tweet_df.shape[0])).copy()\n",
    "    tweet_df['Time'] = pd.to_datetime(tweet_df['Time'])\n",
    "\n",
    "    screen_name_index = {}\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        screen_name_index[row['ScreenName']] = index\n",
    "    tweet_df['ParentDfIndex'] = tweet_df['ParentScreenName'].map(screen_name_index)\n",
    "    tweet_df.fillna(0)\n",
    "    tweet_dfs.append(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_deltas(tweet_df):\n",
    "    t_x = tweet_df.values[-1][5]\n",
    "    time_deltas = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if  row['UserId'] != 'None':\n",
    "            time_delta = t_x - row['Time']\n",
    "            time_deltas.append(time_delta.seconds)\n",
    "    return time_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary of reaction times S_j^x keyed by user id\n",
    "def generate_reaction_times(tweet_df):\n",
    "    reaction_times = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if index > 0 and row['UserId'] != 'None':\n",
    "            reaction_time = row['Time'] - tweet_df.at[row['ParentDfIndex'],\n",
    "                                                      'Time']\n",
    "            reaction_times.append(reaction_time)\n",
    "    return reaction_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary of M_j^x keyed by user id\n",
    "def generate_number_of_follower_who_retweet(tweet_df):\n",
    "    number_of_follower_who_retweet = {}\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if row['UserId'] not in number_of_follower_who_retweet:\n",
    "            number_of_follower_who_retweet[row['UserId']] = 0\n",
    "        parent_user_id = tweet_df.at[row['ParentDfIndex'], 'UserId']\n",
    "        number_of_follower_who_retweet[parent_user_id] += 1\n",
    "    return number_of_follower_who_retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_info(tweet_df, rt_dic):\n",
    "    depth = []\n",
    "    num_followers = []\n",
    "    num_retweets = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if row['UserId'] != 'None':\n",
    "            depth.append(row['DistanceFromRoot'])\n",
    "            if row['FollowerCount'] == 'None':\n",
    "                num_followers.append(0)\n",
    "            else:\n",
    "                num_followers.append(int(row['FollowerCount']))\n",
    "            if row['UserId'] in rt_dic:\n",
    "                num_retweets.append(rt_dic[row['UserId']])\n",
    "            else:\n",
    "                num_reweets.append(0)\n",
    "    return depth, num_followers, num_retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {}\n",
    "log_s_j_x = []\n",
    "# These three are parallel arrays for each retweet \n",
    "# (depth[i], num_followers[i], num_retweets[i] all refer to the same retweet)\n",
    "depth = []\n",
    "num_followers = []\n",
    "num_retweets = []\n",
    "\n",
    "# Prediction arrays\n",
    "t = []\n",
    "T = []\n",
    "S_x = []\n",
    "m_t = []\n",
    "d = []\n",
    "f_x = []\n",
    "for i in range(len(root_tweet_names)):\n",
    "    if partition_assignment[i]:\n",
    "        s_j_x = generate_reaction_times(tweet_dfs[i])\n",
    "        log_s_j_x.append([np.log(i.seconds) for i in s_j_x])\n",
    "        M_j_dic = generate_number_of_follower_who_retweet(tweet_dfs[i])\n",
    "        d_x, M_j_x, m_j_x = generate_graph_info(tweet_dfs[i], M_j_dic)\n",
    "        depth.extend(d_x)\n",
    "        num_followers.extend(M_j_x)\n",
    "        num_retweets.extend(m_j_x)\n",
    "    else:\n",
    "        T.append(generate_time_deltas(tweet_dfs[i]))\n",
    "        t.append(max(T[-1]))\n",
    "        S_j_x = [0]\n",
    "        S_j_x.extend([i.seconds for i in generate_reaction_times(tweet_dfs[i])])\n",
    "        S_x.append(S_j_x)\n",
    "        M_j_dic = generate_number_of_follower_who_retweet(tweet_dfs[i])\n",
    "        d_x, M_j_x, m_j_x = generate_graph_info(tweet_dfs[i], M_j_dic)        \n",
    "        d.append(d_x)\n",
    "        f_x.append(M_j_x)\n",
    "        m_t.append(m_j_x)\n",
    "depth = np.array(depth)\n",
    "num_followers = np.array(num_followers)\n",
    "num_retweets = np.array(num_retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha_48, tau_squared_48_log__, alpha_46, tau_squared_46_log__, alpha_45, tau_squared_45_log__, alpha_39, tau_squared_39_log__, alpha_36, tau_squared_36_log__, alpha_34, tau_squared_34_log__, alpha_32, tau_squared_32_log__, alpha_31, tau_squared_31_log__, alpha_30, tau_squared_30_log__, alpha_29, tau_squared_29_log__, alpha_28, tau_squared_28_log__, alpha_26, tau_squared_26_log__, alpha_25, tau_squared_25_log__, alpha_22, tau_squared_22_log__, alpha_20, tau_squared_20_log__, alpha_19, tau_squared_19_log__, alpha_16, tau_squared_16_log__, alpha_13, tau_squared_13_log__, alpha_11, tau_squared_11_log__, alpha_10, tau_squared_10_log__, alpha_7, tau_squared_7_log__, alpha_6, tau_squared_6_log__, alpha_4, tau_squared_4_log__, alpha_3, tau_squared_3_log__, alpha_2, tau_squared_2_log__, alpha_0, tau_squared_0_log__, b_tau_log__, log_a_tau, sigma_squared_delta_log__, alpha]\n",
      "100%|██████████| 3000/3000 [01:15<00:00, 39.81it/s]\n",
      "There were 13 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 1 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 1 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    }
   ],
   "source": [
    "# Training on the time-related hyperparameters\n",
    "with pm.Model() as time_training:\n",
    "    # global model parameters\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=100)\n",
    "    sigma_squared_delta = pm.InverseGamma('sigma_squared_delta', alpha=2, beta=2)\n",
    "    log_a_tau = pm.Normal('log_a_tau', mu=0, sd=10)\n",
    "    b_tau = pm.Gamma('b_tau', alpha=1, beta=.002)\n",
    "    \n",
    "    # log-normal model for reaction times, nonrecursive...\n",
    "    a_tau = pm.Deterministic('a_tau', pm.math.exp(log_a_tau))\n",
    "    \n",
    "    i_prime = 0\n",
    "    for i in range(num_root_tweets):\n",
    "        if partition_assignment[i]:\n",
    "            t_x = pm.InverseGamma('tau_squared_{}'.format(i), alpha=a_tau, beta=b_tau)\n",
    "            a_x = pm.Normal('alpha_{}'.format(i), mu=alpha, tau=1/sigma_squared_delta)        \n",
    "            l_x = pm.Normal('log_s_{}'.format(i), mu=a_x, sd=t_x**0.5, observed=log_s_j_x[i_prime])\n",
    "            i_prime += 1\n",
    "# Run and fit our model\n",
    "with time_training:\n",
    "    trace = pm.sample(1000, tune=2000, cores=4)\n",
    "    time_params = ['alpha', 'sigma_squared_delta', 'a_tau', 'b_tau']\n",
    "    # Extract the hyperparameters\n",
    "    for param in time_params:\n",
    "        hyperparams[param] = np.mean(trace[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [logit_b_j, beta_d, beta_F, beta_0, sigma_squared_b_log__]\n",
      "100%|██████████| 2000/2000 [05:33<00:00,  6.00it/s]\n",
      "The acceptance probability does not match the target. It is 0.904375304915, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    }
   ],
   "source": [
    "# Training on the graph related hyperparameters\n",
    "with pm.Model() as graph_training:\n",
    "    sigma_squared_b = pm.InverseGamma('sigma_squared_b', alpha=0.5, beta=0.5, testval=10000)\n",
    "    beta_0 = pm.Normal('beta_0', mu=0, tau=1/10000, testval=1.99)\n",
    "    beta_F = pm.Normal('beta_F', mu=0, tau=1/10000, testval=-0.79)\n",
    "    beta_d = pm.Normal('beta_d', mu=0, tau=1/10000)\n",
    "    \n",
    "    u_j = beta_0 + beta_F * pm.math.log(num_followers+1) + beta_d * pm.math.log(depth+1)\n",
    "    logit_b_j = pm.Normal('logit_b_j', mu=u_j, tau=1/sigma_squared_b, shape=len(depth))\n",
    "    b_j = pm.math.invlogit(logit_b_j)\n",
    "    M_j = pm.Binomial('retweet_count M_j', n=num_followers, p=b_j, observed=num_retweets)\n",
    "    \n",
    "    \n",
    "# Run and fit our model\n",
    "with graph_training:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)\n",
    "    graph_params = ['sigma_squared_b', 'beta_0', 'beta_F', 'beta_d']\n",
    "    # Extract the hyperparameters\n",
    "    for param in graph_params:\n",
    "        hyperparams[param] = np.mean(trace[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph all of the important distributions for time hyper-paramers\n",
    "with time_model:\n",
    "    imp_dists = ['alpha', 'sigma_squared_delta', 'a_tau', 'b_tau']\n",
    "#     print(np.mean(trace[imp_dists[0]]))\n",
    "    fig, axs = plt.subplots(2,2, figsize = (8,8))\n",
    "    for i in range(len(imp_dists)):\n",
    "        ax1 = axs[int(i/2)][i%2]\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set(ylim=(-.25, .25))\n",
    "        var = imp_dists[i]\n",
    "        sns.distplot(trace[var], ax=ax1).set_title(var);\n",
    "        sns.boxplot(trace[var], ax=ax2, notch=True)\n",
    "        print(var + \": \" + str(np.mean(trace[var])) + \"(\" + str(np.std(trace[var])) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# graph all of the important distributions for graph hyper-paramers\n",
    "with graph_model:\n",
    "    imp_dists = ['sigma_squared_b', 'beta_0', 'beta_F', 'beta_d']\n",
    "#    fig, axs = plt.subplots(2,2, figsize = (8,8))\n",
    "    for i in range(len(imp_dists)):\n",
    "#         ax1 = axs[int(i/2)][i%2]\n",
    "#         ax2 = ax1.twinx()\n",
    "#         ax2.set(ylim=(-.25, .25))\n",
    "        var = imp_dists[i]\n",
    "#         sns.distplot(trace[var], ax=ax1).set_title(var);\n",
    "#         sns.boxplot(trace[var], ax=ax2, notch=True)\n",
    "        print(var + \": \" + str(np.mean(trace[var])) + \"(\" + str(np.std(trace[var])) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha_51, tau_squared_51_log__, alpha_50, tau_squared_50_log__, alpha_49, tau_squared_49_log__, alpha_47, tau_squared_47_log__, alpha_44, tau_squared_44_log__, alpha_43, tau_squared_43_log__, alpha_42, tau_squared_42_log__, alpha_41, tau_squared_41_log__, alpha_40, tau_squared_40_log__, alpha_38, tau_squared_38_log__, alpha_37, tau_squared_37_log__, alpha_35, tau_squared_35_log__, alpha_33, tau_squared_33_log__, alpha_27, tau_squared_27_log__, alpha_24, tau_squared_24_log__, alpha_23, tau_squared_23_log__, alpha_21, tau_squared_21_log__, alpha_18, tau_squared_18_log__, alpha_17, tau_squared_17_log__, alpha_15, tau_squared_15_log__, alpha_14, tau_squared_14_log__, alpha_12, tau_squared_12_log__, alpha_9, tau_squared_9_log__, alpha_8, tau_squared_8_log__, alpha_5, tau_squared_5_log__, alpha_1, tau_squared_1_log__]\n",
      "100%|██████████| 2000/2000 [00:24<00:00, 81.65it/s]\n"
     ]
    }
   ],
   "source": [
    "a_x_prediction = []\n",
    "t_x_prediction = []\n",
    "# Sample the a_x, t_x latent variables for the prediction tweets\n",
    "with pm.Model() as time_prediction:\n",
    "    i_prime = 0\n",
    "    for i in range(num_root_tweets):\n",
    "        if not partition_assignment[i]:\n",
    "            t_x = pm.InverseGamma('tau_squared_{}'.format(i), alpha=hyperparams['a_tau'], beta=hyperparams['b_tau'])\n",
    "            a_x = pm.Normal('alpha_{}'.format(i), mu=hyperparams['alpha'], tau=1/hyperparams['sigma_squared_delta'])        \n",
    "            l_x = pm.Normal('log_{}'.format(i), mu=a_x, sd=t_x**0.5, observed=log_s_j_x[i_prime])\n",
    "            i_prime += 1\n",
    "        \n",
    "# Run and fit our model\n",
    "with time_prediction:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)\n",
    "    for i in range(num_root_tweets):\n",
    "        if not partition_assignment[i]:\n",
    "            a_x_prediction.append(np.mean(trace['alpha_{}'.format(i)]))\n",
    "            t_x_prediction.append(np.mean(trace['tau_squared_{}'.format(i)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retweet(pm.Discrete):\n",
    "    def __init__(self, alpha_x, t_x, S_x, m_t, t, b_x, f_x, *args, **kwargs):\n",
    "        super(Retweet, self).__init__(*args, **kwargs)\n",
    "        self.alpha_x = alpha_x\n",
    "        self.t_x = t_x\n",
    "        self.S_x = S_x\n",
    "        self.m_t = m_t\n",
    "        self.t = t\n",
    "        self.b_x = b_x\n",
    "        self.f_x = f_x\n",
    "        self.i = i\n",
    "    \n",
    "    def logp(self, M):\n",
    "        if pm.math.lt(M ,self.m_t):\n",
    "            return -np.inf\n",
    "        \n",
    "        nCr = lambda x,y: math.factorial(x) / (math.factorial(y) * math.factorial(x - y))\n",
    "        \n",
    "        choose_term_1 = nCr(M, self.m_t)   \n",
    "        gauss = scipy.stats.norm(self.alpha_x, self.t_x)\n",
    "        f_term = gauss.cdf(1 - math.log(self.t - self.S_x)) ** (M - self.m_t)\n",
    "        choose_term_2 = nCr(self.f_x, M)\n",
    "        binomial_term = (self.b ** M) * (1 - b) ** (f_x - M)\n",
    "\n",
    "        return math.log(choose_term_1 * f_term * choose_term_2 * binomial_term) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped from one run\n",
    "hyperparams = {}\n",
    "hyperparams['alpha'] = 7.3624630711436341\n",
    "hyperparams['sigma_squared_delta'] = 0.58742005712857215\n",
    "hyperparams['a_tau'] = 5.5645302260927982\n",
    "hyperparams['b_tau'] = 19.311009905771179\n",
    "hyperparams['sigma_squared_b'] = 5.1719876665358724\n",
    "hyperparams['beta_0'] = -7.9057724341651188\n",
    "hyperparams['beta_F'] = -0.089856350130540039\n",
    "hyperparams['beta_d'] = -6.2727785270466097\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as graph_prediction:\n",
    "    for i in range(len(T)):\n",
    "        # TODO: Generate S_x, m_t, t for each prediction tweet\n",
    "        for j in range(len(T[i])):\n",
    "            u_j = hyperparams['beta_0'] + hyperparams['beta_F'] * pm.math.log(f_x[i][j]+1) + hyperparams['beta_d'] * pm.math.log(d[i][j]+1)\n",
    "            logit_b_j = pm.Normal('logit_b_{}_{}'.format(i,j), mu=u_j, tau=1/hyperparams['sigma_squared_b'], shape=len(depth))\n",
    "            b_j = pm.math.invlogit(logit_b_j)\n",
    "            likelihood = Retweet('likelihood_{}_{}'.format(i,j), alpha_x = a_x_prediction[i], t_x = t_x_prediction[i], S_x = S_x[i][j], m_t = m_t[i][j], t = t[i], b_x=b_j, f_x = f_x[i][j], testval=m_t[i][j])\n",
    "        \n",
    "with graph_prediction:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnusedInputError",
     "evalue": "theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: likelihood_0_0_shared__.\nTo make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function. To disable it completely, use on_unused_input='ignore'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnusedInputError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-025ab232a33c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph_prediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(draws, step, init, n_init, start, trace, chain_idx, chains, njobs, tune, nuts_kwargs, step_kwargs, progressbar, model, random_seed, live_plot, discard_tuned_samples, live_plot_kwargs, compute_convergence_checks, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_step_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_step_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36massign_step_methods\u001b[0;34m(model, step, methods, step_kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mselected_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minstantiate_steppers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36minstantiate_steppers\u001b[0;34m(model, steps, selected_steps, step_kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mused_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/step_methods/hmc/nuts.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vars, max_treedepth, early_max_treedepth, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdesired\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtuning\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \"\"\"\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_treedepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_treedepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/step_methods/hmc/base_hmc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vars, scaling, step_scale, is_cov, model, blocked, potential, integrator, dtype, Emax, target_accept, gamma, k, t0, adapt_step_size, step_rand, **theano_kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         super(BaseHMC, self).__init__(vars, blocked=blocked, model=model,\n\u001b[0;32m---> 63\u001b[0;31m                                       dtype=dtype, **theano_kwargs)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_step_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapt_step_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/step_methods/arraystep.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vars, model, blocked, dtype, **theano_kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         self._logp_dlogp_func = model.logp_dlogp_function(\n\u001b[0;32m--> 215\u001b[0;31m             vars, dtype=dtype, **theano_kwargs)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/model.py\u001b[0m in \u001b[0;36mlogp_dlogp_function\u001b[0;34m(self, grad_vars, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mvarnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mextra_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_RVs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvarnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mValueGradFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cost, grad_vars, extra_vars, dtype, casting, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         self._theano_function = theano.function(\n\u001b[0;32m--> 405\u001b[0;31m             inputs, [self._cost_joined, grad], givens=givens, **kwargs)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_extra_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    315\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1837\u001b[0m                   \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m                   \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys, name)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;31m# Check if some input variables are unused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_unused_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0;31m# Make a list of (SymbolicInput|SymblicInputKits, indices,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m_check_unused_inputs\u001b[0;34m(self, inputs, outputs, on_unused_input)\u001b[0m\n\u001b[1;32m   1625\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mon_unused_input\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                     raise UnusedInputError(msg % (inputs.index(i),\n\u001b[0;32m-> 1627\u001b[0;31m                                                   i.variable, err_msg))\n\u001b[0m\u001b[1;32m   1628\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     raise ValueError(\"Invalid value for keyword \"\n",
      "\u001b[0;31mUnusedInputError\u001b[0m: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: likelihood_0_0_shared__.\nTo make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function. To disable it completely, use on_unused_input='ignore'."
     ]
    }
   ],
   "source": [
    "with pm.Model() as graph_prediction:\n",
    "    i = 0\n",
    "    # TODO: Generate S_x, m_t, t for each prediction tweet\n",
    "    for j in range(len(T[i])):\n",
    "        u_j = hyperparams['beta_0'] + hyperparams['beta_F'] * pm.math.log(f_x[i][j]+1) + hyperparams['beta_d'] * pm.math.log(d[i][j]+1)\n",
    "        logit_b_j = pm.Normal('logit_b_{}_{}'.format(i,j), mu=u_j, tau=1/hyperparams['sigma_squared_b'], shape=len(depth))\n",
    "        b_j = pm.math.invlogit(logit_b_j)\n",
    "        likelihood = Retweet('likelihood_{}_{}'.format(i,j), alpha_x = a_x_prediction[i], t_x = t_x_prediction[i], S_x = S_x[i][j], m_t = m_t[i][j], t = t[i], b_x=b_j, f_x = f_x[i][j], testval=m_t[i][j])\n",
    "        \n",
    "with graph_prediction:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
