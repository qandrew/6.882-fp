{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import scipy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xlrd\n",
    "from theano.compile.ops import as_op\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../RetweetDataAOAS/retweet_data/'\n",
    "root_tweet_names = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "num_root_tweets = len(root_tweet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_name_to_index = {}\n",
    "for i in range(num_root_tweets):\n",
    "    tweet_name_to_index[root_tweet_names[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_partition_file_name(name):\n",
    "    root = name.split('.')\n",
    "    items = root[0].split('_')\n",
    "    items[-2], items[-1] = items[-1], items[-2]\n",
    "    return \".\".join([\"_\".join(items), root[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_path = '../Partition_1.xlsx'\n",
    "partition = pd.read_excel(partition_path)\n",
    "partition_assignment = {}\n",
    "for index, row in partition.iterrows():\n",
    "    training_file_name = format_partition_file_name(row['Training'])\n",
    "    prediction_file_name = format_partition_file_name(row['Prediction'])\n",
    "    partition_assignment[tweet_name_to_index[training_file_name]] = True\n",
    "    partition_assignment[tweet_name_to_index[prediction_file_name]] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces a dictionary of dataframes for each tweetfile, with initial \n",
    "# preprocessing\n",
    "observation_probability = 1\n",
    "fields = ['RetweetCount', 'UserId', 'ScreenName', 'FollowerCount', \n",
    "          'DistanceFromRoot','Time', 'ParentScreenName', 'Text']\n",
    "tweet_dfs = []\n",
    "for i in range(num_root_tweets):\n",
    "    tweet_df = pd.read_csv(path+root_tweet_names[i], sep=\"\\t\", header=None, \n",
    "                         quoting=csv.QUOTE_NONE, names=fields, encoding = \"ISO-8859-1\")\n",
    "    if not partition_assignment[i]:\n",
    "        tweet_df = tweet_df.head(int(observation_probability * tweet_df.shape[0])).copy()\n",
    "    tweet_df['Time'] = pd.to_datetime(tweet_df['Time'])\n",
    "\n",
    "    screen_name_index = {}\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        screen_name_index[row['ScreenName']] = index\n",
    "    tweet_df['ParentDfIndex'] = tweet_df['ParentScreenName'].map(screen_name_index)\n",
    "    tweet_df.fillna(0)\n",
    "    tweet_dfs.append(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_deltas(tweet_df):\n",
    "    t_x = tweet_df.values[-1][5]\n",
    "    time_deltas = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if  row['UserId'] != 'None':\n",
    "            time_delta = t_x - row['Time']\n",
    "            time_deltas.append(time_delta.seconds)\n",
    "    return time_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary of reaction times S_j^x keyed by user id\n",
    "def generate_reaction_times(tweet_df):\n",
    "    reaction_times = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if index > 0 and row['UserId'] != 'None':\n",
    "            reaction_time = row['Time'] - tweet_df.at[row['ParentDfIndex'],\n",
    "                                                      'Time']\n",
    "            reaction_times.append(reaction_time)\n",
    "    return reaction_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary of M_j^x keyed by user id\n",
    "def generate_number_of_follower_who_retweet(tweet_df):\n",
    "    number_of_follower_who_retweet = {}\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if row['UserId'] not in number_of_follower_who_retweet:\n",
    "            number_of_follower_who_retweet[row['UserId']] = 0\n",
    "        parent_user_id = tweet_df.at[row['ParentDfIndex'], 'UserId']\n",
    "        number_of_follower_who_retweet[parent_user_id] += 1\n",
    "    return number_of_follower_who_retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_info(tweet_df, rt_dic):\n",
    "    depth = []\n",
    "    num_followers = []\n",
    "    num_retweets = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if row['UserId'] != 'None':\n",
    "            depth.append(row['DistanceFromRoot'])\n",
    "            if row['FollowerCount'] == 'None':\n",
    "                num_followers.append(0)\n",
    "            else:\n",
    "                num_followers.append(int(row['FollowerCount']))\n",
    "            if row['UserId'] in rt_dic:\n",
    "                num_retweets.append(rt_dic[row['UserId']])\n",
    "            else:\n",
    "                num_reweets.append(0)\n",
    "    return depth, num_followers, num_retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {}\n",
    "log_s_j_x = []\n",
    "# These three are parallel arrays for each retweet \n",
    "# (depth[i], num_followers[i], num_retweets[i] all refer to the same retweet)\n",
    "depth = []\n",
    "num_followers = []\n",
    "num_retweets = []\n",
    "\n",
    "# Prediction arrays\n",
    "t = []\n",
    "T = []\n",
    "S_x = []\n",
    "m_t = []\n",
    "d = []\n",
    "f_x = []\n",
    "for i in range(len(root_tweet_names)):\n",
    "    if partition_assignment[i]:\n",
    "        s_j_x = generate_reaction_times(tweet_dfs[i])\n",
    "        log_s_j_x.append([np.log(i.seconds) for i in s_j_x])\n",
    "        M_j_dic = generate_number_of_follower_who_retweet(tweet_dfs[i])\n",
    "        d_x, M_j_x, m_j_x = generate_graph_info(tweet_dfs[i], M_j_dic)\n",
    "        depth.extend(d_x)\n",
    "        num_followers.extend(M_j_x)\n",
    "        num_retweets.extend(m_j_x)\n",
    "    else:\n",
    "        T.append(generate_time_deltas(tweet_dfs[i]))\n",
    "        t.append(max(T[-1]))\n",
    "        S_j_x = [0]\n",
    "        S_j_x.extend([i.seconds for i in generate_reaction_times(tweet_dfs[i])])\n",
    "        S_x.append(S_j_x)\n",
    "        M_j_dic = generate_number_of_follower_who_retweet(tweet_dfs[i])\n",
    "        d_x, M_j_x, m_j_x = generate_graph_info(tweet_dfs[i], M_j_dic)        \n",
    "        d.append(d_x)\n",
    "        f_x.append(M_j_x)\n",
    "        m_t.append(m_j_x)\n",
    "depth = np.array(depth)\n",
    "num_followers = np.array(num_followers)\n",
    "num_retweets = np.array(num_retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {}\n",
    "# Training on the time-related hyperparameters\n",
    "with pm.Model() as time_training:\n",
    "    # global model parameters\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=100)\n",
    "    sigma_squared_delta = pm.InverseGamma('sigma_squared_delta', alpha=2, beta=2)\n",
    "    log_a_tau = pm.Normal('log_a_tau', mu=0, sd=10)\n",
    "    b_tau = pm.Gamma('b_tau', alpha=1, beta=.002)\n",
    "    \n",
    "    # log-normal model for reaction times, nonrecursive...\n",
    "    a_tau = pm.Deterministic('a_tau', pm.math.exp(log_a_tau))\n",
    "    \n",
    "    i_prime = 0\n",
    "    for i in range(num_root_tweets):\n",
    "        if partition_assignment[i]:\n",
    "            t_x = pm.InverseGamma('tau_squared_{}'.format(i), alpha=a_tau, beta=b_tau)\n",
    "            a_x = pm.Normal('alpha_{}'.format(i), mu=alpha, tau=1/sigma_squared_delta)        \n",
    "            l_x = pm.Normal('log_s_{}'.format(i), mu=a_x, sd=t_x**0.5, observed=log_s_j_x[i_prime])\n",
    "            i_prime += 1\n",
    "# Run and fit our model\n",
    "with time_training:\n",
    "    trace = pm.sample(1000, tune=2000, cores=4)\n",
    "    time_params = ['alpha', 'sigma_squared_delta', 'a_tau', 'b_tau']\n",
    "    # Extract the hyperparameters\n",
    "    for param in time_params:\n",
    "        hyperparams[param] = trace[param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the graph related hyperparameters\n",
    "with pm.Model() as graph_training:\n",
    "    sigma_squared_b = pm.InverseGamma('sigma_squared_b', alpha=0.5, beta=0.5, testval=10000)\n",
    "    beta_0 = pm.Normal('beta_0', mu=0, tau=1/10000, testval=1.99)\n",
    "    beta_F = pm.Normal('beta_F', mu=0, tau=1/10000, testval=-0.79)\n",
    "    beta_d = pm.Normal('beta_d', mu=0, tau=1/10000)\n",
    "    \n",
    "    u_j = beta_0 + beta_F * pm.math.log(num_followers+1) + beta_d * pm.math.log(depth+1)\n",
    "    logit_b_j = pm.Normal('logit_b_j', mu=u_j, tau=1/sigma_squared_b, shape=len(depth))\n",
    "    b_j = pm.math.invlogit(logit_b_j)\n",
    "    M_j = pm.Binomial('retweet_count M_j', n=num_followers, p=b_j, observed=num_retweets)\n",
    "    \n",
    "    \n",
    "# Run and fit our model\n",
    "with graph_training:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)\n",
    "    graph_params = ['sigma_squared_b', 'beta_0', 'beta_F', 'beta_d']\n",
    "    # Extract the hyperparameters\n",
    "    for param in graph_params:\n",
    "        hyperparams[param] = trace[param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_params = {}\n",
    "paper_params['alpha'] = 7.42\n",
    "paper_params['sigma_squared_delta'] =  0.65**2\n",
    "paper_params['a_tau'] = 1/0.45\n",
    "paper_params['b_tau'] = 2.11\n",
    "paper_params['sigma_squared_b'] = 1.69 **2\n",
    "paper_params['beta_0'] = -4.61\n",
    "paper_params['beta_F'] = -0.28\n",
    "paper_params['beta_d'] = -8.22\n",
    "scale = {}\n",
    "scale['alpha'] = 3\n",
    "scale['sigma_squared_delta'] =  3.0\n",
    "scale['a_tau'] = 0.30\n",
    "scale['b_tau'] = 0.08\n",
    "scale['sigma_squared_b'] = 0.6\n",
    "scale['beta_0'] = 0.40\n",
    "scale['beta_F'] = 6\n",
    "scale['beta_d'] = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_dists = ['alpha', 'sigma_squared_delta', 'a_tau', 'b_tau', 'sigma_squared_b', 'beta_0', 'beta_F', 'beta_d']\n",
    "fig, axs = plt.subplots(ncols=4, nrows=2, figsize = (17,8))\n",
    "for i in range(len(imp_dists)):\n",
    "    param = imp_dists[i]\n",
    "    ax1 = axs[int(i / 4)][i % 4]\n",
    "    sns.distplot(hyperparams[param], ax=ax1).set_title(param)\n",
    "    ax1.plot([paper_params[param] for i in range(11)], [scale[param] *i/10 for i in range(11)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_x_prediction = []\n",
    "t_x_prediction = []\n",
    "# Sample the a_x, t_x latent variables for the prediction tweets\n",
    "with pm.Model() as time_prediction:\n",
    "    i_prime = 0\n",
    "    for i in range(num_root_tweets):\n",
    "        if not partition_assignment[i]:\n",
    "            t_x = pm.InverseGamma('tau_squared_{}'.format(i), alpha=1/paper_params['a_tau'], beta=paper_params['b_tau'])\n",
    "            a_x = pm.Normal('alpha_{}'.format(i), mu=paper_params['alpha'], tau=1/(paper_params['sigma_squared_delta']))        \n",
    "            l_x = pm.Normal('log_{}'.format(i), mu=a_x, sd=t_x**0.5, observed=log_s_j_x[i_prime])\n",
    "            i_prime += 1\n",
    "        \n",
    "# Run and fit our model\n",
    "with time_prediction:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)\n",
    "    for i in range(num_root_tweets):\n",
    "        if not partition_assignment[i]:\n",
    "            a_x_prediction.append(np.mean(trace['alpha_{}'.format(i)]))\n",
    "            t_x_prediction.append(np.mean(trace['tau_squared_{}'.format(i)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retweet(pm.Discrete):\n",
    "    def __init__(self, alpha_x, t_x, S_x, m_t, t, b_x, f_x, *args, **kwargs):\n",
    "        super(Retweet, self).__init__(*args, **kwargs)\n",
    "        self.alpha_x = alpha_x\n",
    "        self.t_x = t_x\n",
    "        self.S_x = S_x\n",
    "        self.m_t = m_t\n",
    "        self.t = t\n",
    "        self.b_x = b_x\n",
    "        self.f_x = f_x\n",
    "    \n",
    "    def logp(self, M):\n",
    "        nCr = lambda x,y: np.math.factorial(x) / (np.math.factorial(y) * np.math.factorial(x - y))\n",
    "        \n",
    "        choose_term_1 = nCr(M, self.m_t)   \n",
    "        gauss = scipy.stats.norm(self.alpha_x, self.t_x)\n",
    "        f_term = gauss.cdf(1 - math.log(self.t - self.S_x)) ** (M - self.m_t)\n",
    "        choose_term_2 = nCr(self.f_x, M)\n",
    "        binomial_term = (self.b ** M) * (1 - b) ** (self.f_x - M)\n",
    "\n",
    "        return np.log(choose_term_1 * f_term * choose_term_2 * binomial_term) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stirling_approximation(n):\n",
    "    if (n == 0):\n",
    "        return 0\n",
    "    t_1 = np.log(2 * np.pi * n) * 1/2\n",
    "    t_2 = np.log(n/np.e) * n\n",
    "    return t_1 + t_2\n",
    "\n",
    "log_ncr = lambda a,b: 0 if a==b else log_stirling_approximation(a) - log_stirling_approximation(a-b) - log_stirling_approximation(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Discrete-Value Slice Sampler MCMC Method that uses log-probability to sample from a distribution\n",
    "class LogDiscreteSliceSampler():\n",
    "    def __init__(self, log_p, init_val,logit_b):\n",
    "        self.log_p = logp\n",
    "        self.last_sample = int(init_val)\n",
    "        self.logit_b = logit_b\n",
    "        self.step_size = 1\n",
    "        \n",
    "    def sample(self, iternum):\n",
    "        samples = []\n",
    "        b = scipy.special.expit(np.random.normal(loc=logit_b_j, scale=1.69))\n",
    "        for i in range(iternum):\n",
    "            y = np.log(np.random.uniform(low = 0,high=1)) + self.log_p(self.last_sample,b)\n",
    "            left = self.last_sample - self.step_size\n",
    "            right = self.last_sample + self.step_size\n",
    "            while (self.log_p(left,b) > y):\n",
    "                left = left - self.step_size\n",
    "            while (self.log_p(right,b) > y):\n",
    "                right = right + self.step_size\n",
    "            sample = int(np.random.uniform(low = left, high = right))\n",
    "            while (self.log_p(sample,b) < y):\n",
    "                sample = int(np.random.uniform(low = left, high = right))\n",
    "            samples.append(sample)\n",
    "            self.last_sample = sample\n",
    "        return samples\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_samples = []\n",
    "for i in range(len(f_x)):\n",
    "    print(\"STARTING.... \" + str(i))\n",
    "    gauss = scipy.stats.norm(a_x_prediction[i], t_x_prediction[i])\n",
    "    M_j_samples = []\n",
    "    M_samples.append(M_j_samples)\n",
    "    for j in range(len(f_x[i])):\n",
    "        if f_x[i][j] == 0:\n",
    "            M_j_samples.append([0])\n",
    "            continue\n",
    "        logit_b_j = paper_params['beta_0'] + paper_params['beta_F'] * np.log(f_x[i][j]+1) + paper_params['beta_d'] * np.log(d[i][j]+1)\n",
    "        def logp(M, b):\n",
    "            if M < m_t[i][j] or M > f_x[i][j]:\n",
    "                return -np.inf\n",
    "            log_choose_term_1 = log_ncr(M, m_t[i][j]) \n",
    "            log_choose_term_2 = log_ncr(f_x[i][j], M)\n",
    "\n",
    "            log_gauss = np.log(gauss.cdf(1 - np.log(t[i] - S_x[i][j])))\n",
    "            log_f_term =  log_gauss * (M - m_t[i][j])\n",
    "\n",
    "            log_binomial_term = np.log(b) * M + np.log(1 - b)*(f_x[i][j] - M)\n",
    "            full_term = log_choose_term_1 + log_choose_term_2 + log_binomial_term\n",
    "            return full_term\n",
    "        init_logp = logp(m_t[i][j],scipy.special.expit(logit_b_j))\n",
    "        logp_norm = lambda x,b: logp(x,b) * init_logp\n",
    "        \n",
    "        sampler = LogDiscreteSliceSampler(logp_norm, m_t[i][j], logit_b_j)\n",
    "        M_j_samples.append(sampler.sample(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.sum(i) for i in m_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =np.divide(np.array([int(np.sum([np.mean(j) for j in i])) for i in M_samples]) - np.array([np.sum(i) for i in m_t]), np.array([np.sum(i) for i in m_t]))\n",
    "a = np.absolute(a)\n",
    "print(np.median(a))\n",
    "print(np.std(a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused code trying to manually find p(M|m,a,t)\n",
    "M_probs = []\n",
    "for i in range(1):\n",
    "    gauss = scipy.stats.norm(a_x_prediction[i], t_x_prediction[i])\n",
    "    M_j_probs = []\n",
    "    for j in range(1):\n",
    "        logit_b_j = hyperparams['beta_0'] + hyperparams['beta_F'] * np.log(f_x[i][j]+1) + hyperparams['beta_d'] * np.log(d[i][j]+1)\n",
    "        b_j = scipy.special.expit(logit_b_j)\n",
    "        M = np.arange(f_x[i][j]+1)\n",
    "        M_j_k_prob = np.ones(f_x[i][j]+1) * -np.inf\n",
    "\n",
    "\n",
    "        log_choose_term_1 = log_ncr(M, m_t[i][j]) \n",
    "        log_choose_term_2 = log_ncr(f_x[i][j], M)\n",
    "\n",
    "        log_gauss = np.log(gauss.cdf(1 - np.log(t[i] - S_x[i][j])))\n",
    "        log_f_term =  log_gauss * (M - m_t[i][j])\n",
    "\n",
    "        log_binomial_term = np.log(b_j) * M + np.log(1 - b_j)*(f_x[i][j] - M)\n",
    "        full_term = log_choose_term_1 + log_f_term + log_choose_term_2 + log_binomial_term\n",
    "        print(np.nanmax(full_term))\n",
    "        M_j_k_prob[M > m_t[i][j]] = full_term[M > m_t[i][j]]\n",
    "        M_j_k_prob[np.where(np.isnan(M_j_k_prob))] = -np.inf\n",
    "        M_j_k_prob = np.exp(M_j_k_prob - np.amax(M_j_k_prob))\n",
    "        M_j_k_prob = M_j_k_prob / np.sum(M_j_k_prob)\n",
    "        M_j_probs.append(M_j_k_prob)\n",
    "    M_probs.append(M_j_probs)\n",
    "M_probs = np.array(M_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
