{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import scipy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xlrd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../RetweetDataAOAS/retweet_data/'\n",
    "root_tweet_names = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "num_root_tweets = len(root_tweet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_name_to_index = {}\n",
    "for i in range(num_root_tweets):\n",
    "    tweet_name_to_index[root_tweet_names[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_partition_file_name(name):\n",
    "    root = name.split('.')\n",
    "    items = root[0].split('_')\n",
    "    items[-2], items[-1] = items[-1], items[-2]\n",
    "    return \".\".join([\"_\".join(items), root[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_path = '../Partition_1.xlsx'\n",
    "partition = pd.read_excel(partition_path)\n",
    "partition_assignment = {}\n",
    "for index, row in partition.iterrows():\n",
    "    training_file_name = format_partition_file_name(row['Training'])\n",
    "    prediction_file_name = format_partition_file_name(row['Prediction'])\n",
    "    partition_assignment[tweet_name_to_index[training_file_name]] = True\n",
    "    partition_assignment[tweet_name_to_index[prediction_file_name]] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces a dictionary of dataframes for each tweetfile, with initial \n",
    "# preprocessing\n",
    "observation_probability = .25\n",
    "fields = ['RetweetCount', 'UserId', 'ScreenName', 'FollowerCount', \n",
    "          'DistanceFromRoot','Time', 'ParentScreenName', 'Text']\n",
    "tweet_dfs = []\n",
    "for i in range(num_root_tweets):\n",
    "    tweet_df = pd.read_csv(path+root_tweet_names[i], sep=\"\\t\", header=None, \n",
    "                         quoting=csv.QUOTE_NONE, names=fields, encoding = \"ISO-8859-1\")\n",
    "    if not partition_assignment[i]:\n",
    "        tweet_df = tweet_df.head(int(observation_probability * tweet_df.shape[0])).copy()\n",
    "    tweet_df['Time'] = pd.to_datetime(tweet_df['Time'])\n",
    "\n",
    "    screen_name_index = {}\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        screen_name_index[row['ScreenName']] = index\n",
    "    tweet_df['ParentDfIndex'] = tweet_df['ParentScreenName'].map(screen_name_index)\n",
    "    tweet_df.fillna(0)\n",
    "    tweet_dfs.append(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_deltas(tweet_df):\n",
    "    t_x = tweet_df.values[-1][5]\n",
    "    time_deltas = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        time_delta = t_x - row['Time']\n",
    "        time_deltas.append(time_delta.seconds)\n",
    "    return time_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary of reaction times S_j^x keyed by user id\n",
    "def generate_reaction_times(tweet_df):\n",
    "    reaction_times = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if index > 0:\n",
    "            reaction_time = row['Time'] - tweet_df.at[row['ParentDfIndex'],\n",
    "                                                      'Time']\n",
    "            reaction_times.append(reaction_time)\n",
    "    return reaction_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary of M_j^x keyed by user id\n",
    "def generate_number_of_follower_who_retweet(tweet_df):\n",
    "    number_of_follower_who_retweet = {}\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if row['UserId'] not in number_of_follower_who_retweet:\n",
    "            number_of_follower_who_retweet[row['UserId']] = 0\n",
    "        parent_user_id = tweet_df.at[row['ParentDfIndex'], 'UserId']\n",
    "        number_of_follower_who_retweet[parent_user_id] += 1\n",
    "    return number_of_follower_who_retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_info(tweet_df, rt_dic):\n",
    "    depth = []\n",
    "    num_followers = []\n",
    "    num_retweets = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if row['UserId'] != 'None':\n",
    "            depth.append(row['DistanceFromRoot'])\n",
    "            if row['FollowerCount'] == 'None':\n",
    "                num_followers.append(0)\n",
    "            else:\n",
    "                num_followers.append(int(row['FollowerCount']))\n",
    "            if row['UserId'] in rt_dic:\n",
    "                num_retweets.append(rt_dic[row['UserId']])\n",
    "            else:\n",
    "                num_reweets.append(0)\n",
    "    return depth, num_followers, num_retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {}\n",
    "log_s_j_x = []\n",
    "# These three are parallel arrays for each retweet \n",
    "# (depth[i], num_followers[i], num_retweets[i] all refer to the same retweet)\n",
    "depth = []\n",
    "num_followers = []\n",
    "num_retweets = []\n",
    "\n",
    "# Prediction arrays\n",
    "t = []\n",
    "T = []\n",
    "S_x = []\n",
    "m_t = []\n",
    "d = []\n",
    "f_x = []\n",
    "for i in range(len(root_tweet_names)):\n",
    "    if partition_assignment[i]:\n",
    "        s_j_x = generate_reaction_times(tweet_dfs[i])\n",
    "        log_s_j_x.append([np.log(i.seconds) for i in s_j_x])\n",
    "        M_j_dic = generate_number_of_follower_who_retweet(tweet_dfs[i])\n",
    "        d_x, M_j_x, m_j_x = generate_graph_info(tweet_dfs[i], M_j_dic)\n",
    "        depth.extend(d_x)\n",
    "        num_followers.extend(M_j_x)\n",
    "        num_retweets.extend(m_j_x)\n",
    "    else:\n",
    "        T.append(generate_time_deltas(tweet_dfs[i]))\n",
    "        t.append(max(T[-1]))\n",
    "        S_j_x = [0]\n",
    "        S_j_x.extend([i.seconds for i in generate_reaction_times(tweet_dfs[i])])\n",
    "        S_x.append(S_j_x)\n",
    "        M_j_dic = generate_number_of_follower_who_retweet(tweet_dfs[i])\n",
    "        d_x, M_j_x, m_j_x = generate_graph_info(tweet_dfs[i], M_j_dic)        \n",
    "        d.append(d_x)\n",
    "        f_x.append(M_j_x)\n",
    "        m_t.append(m_j_x)\n",
    "depth = np.array(depth)\n",
    "num_followers = np.array(num_followers)\n",
    "num_retweets = np.array(num_retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1187, 1159, 1141, 1135, 1107, 1076, 1071, 942, 942, 926, 919, 834, 811, 599, 571, 553, 390, 0]\n",
      "1187\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha_48, tau_squared_48_log__, alpha_46, tau_squared_46_log__, alpha_45, tau_squared_45_log__, alpha_39, tau_squared_39_log__, alpha_36, tau_squared_36_log__, alpha_34, tau_squared_34_log__, alpha_32, tau_squared_32_log__, alpha_31, tau_squared_31_log__, alpha_30, tau_squared_30_log__, alpha_29, tau_squared_29_log__, alpha_28, tau_squared_28_log__, alpha_26, tau_squared_26_log__, alpha_25, tau_squared_25_log__, alpha_22, tau_squared_22_log__, alpha_20, tau_squared_20_log__, alpha_19, tau_squared_19_log__, alpha_16, tau_squared_16_log__, alpha_13, tau_squared_13_log__, alpha_11, tau_squared_11_log__, alpha_10, tau_squared_10_log__, alpha_7, tau_squared_7_log__, alpha_6, tau_squared_6_log__, alpha_4, tau_squared_4_log__, alpha_3, tau_squared_3_log__, alpha_2, tau_squared_2_log__, alpha_0, tau_squared_0_log__, b_tau_log__, log_a_tau, sigma_squared_delta_log__, alpha]\n",
      "100%|██████████| 3000/3000 [01:15<00:00, 39.81it/s]\n",
      "There were 13 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 1 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 1 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    }
   ],
   "source": [
    "# Training on the time-related hyperparameters\n",
    "with pm.Model() as time_training:\n",
    "    # global model parameters\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=100)\n",
    "    sigma_squared_delta = pm.InverseGamma('sigma_squared_delta', alpha=2, beta=2)\n",
    "    log_a_tau = pm.Normal('log_a_tau', mu=0, sd=10)\n",
    "    b_tau = pm.Gamma('b_tau', alpha=1, beta=.002)\n",
    "    \n",
    "    # log-normal model for reaction times, nonrecursive...\n",
    "    a_tau = pm.Deterministic('a_tau', pm.math.exp(log_a_tau))\n",
    "    \n",
    "    i_prime = 0\n",
    "    for i in range(num_root_tweets):\n",
    "        if partition_assignment[i]:\n",
    "            t_x = pm.InverseGamma('tau_squared_{}'.format(i), alpha=a_tau, beta=b_tau)\n",
    "            a_x = pm.Normal('alpha_{}'.format(i), mu=alpha, tau=1/sigma_squared_delta)        \n",
    "            l_x = pm.Normal('log_s_{}'.format(i), mu=a_x, sd=t_x**0.5, observed=log_s_j_x[i_prime])\n",
    "            i_prime += 1\n",
    "# Run and fit our model\n",
    "with time_training:\n",
    "    trace = pm.sample(1000, tune=2000, cores=4)\n",
    "    time_params = ['alpha', 'sigma_squared_delta', 'a_tau', 'b_tau']\n",
    "    # Extract the hyperparameters\n",
    "    for param in time_params:\n",
    "        hyperparams[param] = np.mean(trace[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [logit_b_j, beta_d, beta_F, beta_0, sigma_squared_b_log__]\n",
      "100%|██████████| 2000/2000 [05:33<00:00,  6.00it/s]\n",
      "The acceptance probability does not match the target. It is 0.904375304915, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    }
   ],
   "source": [
    "# Training on the graph related hyperparameters\n",
    "with pm.Model() as graph_training:\n",
    "    sigma_squared_b = pm.InverseGamma('sigma_squared_b', alpha=0.5, beta=0.5, testval=10000)\n",
    "    beta_0 = pm.Normal('beta_0', mu=0, tau=1/10000, testval=1.99)\n",
    "    beta_F = pm.Normal('beta_F', mu=0, tau=1/10000, testval=-0.79)\n",
    "    beta_d = pm.Normal('beta_d', mu=0, tau=1/10000)\n",
    "    \n",
    "    u_j = beta_0 + beta_F * pm.math.log(num_followers+1) + beta_d * pm.math.log(depth+1)\n",
    "    logit_b_j = pm.Normal('logit_b_j', mu=u_j, tau=1/sigma_squared_b, shape=len(depth))\n",
    "    b_j = pm.math.invlogit(logit_b_j)\n",
    "    M_j = pm.Binomial('retweet_count M_j', n=num_followers, p=b_j, observed=num_retweets)\n",
    "    \n",
    "    \n",
    "# Run and fit our model\n",
    "with graph_training:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)\n",
    "    graph_params = ['sigma_squared_b', 'beta_0', 'beta_F', 'beta_d']\n",
    "    # Extract the hyperparameters\n",
    "    for param in graph_params:\n",
    "        hyperparams[param] = np.mean(trace[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph all of the important distributions for time hyper-paramers\n",
    "with time_model:\n",
    "    imp_dists = ['alpha', 'sigma_squared_delta', 'a_tau', 'b_tau']\n",
    "#     print(np.mean(trace[imp_dists[0]]))\n",
    "    fig, axs = plt.subplots(2,2, figsize = (8,8))\n",
    "    for i in range(len(imp_dists)):\n",
    "        ax1 = axs[int(i/2)][i%2]\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set(ylim=(-.25, .25))\n",
    "        var = imp_dists[i]\n",
    "        sns.distplot(trace[var], ax=ax1).set_title(var);\n",
    "        sns.boxplot(trace[var], ax=ax2, notch=True)\n",
    "        print(var + \": \" + str(np.mean(trace[var])) + \"(\" + str(np.std(trace[var])) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# graph all of the important distributions for graph hyper-paramers\n",
    "with graph_model:\n",
    "    imp_dists = ['sigma_squared_b', 'beta_0', 'beta_F', 'beta_d']\n",
    "#    fig, axs = plt.subplots(2,2, figsize = (8,8))\n",
    "    for i in range(len(imp_dists)):\n",
    "#         ax1 = axs[int(i/2)][i%2]\n",
    "#         ax2 = ax1.twinx()\n",
    "#         ax2.set(ylim=(-.25, .25))\n",
    "        var = imp_dists[i]\n",
    "#         sns.distplot(trace[var], ax=ax1).set_title(var);\n",
    "#         sns.boxplot(trace[var], ax=ax2, notch=True)\n",
    "        print(var + \": \" + str(np.mean(trace[var])) + \"(\" + str(np.std(trace[var])) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha_51, tau_squared_51_log__, alpha_50, tau_squared_50_log__, alpha_49, tau_squared_49_log__, alpha_47, tau_squared_47_log__, alpha_44, tau_squared_44_log__, alpha_43, tau_squared_43_log__, alpha_42, tau_squared_42_log__, alpha_41, tau_squared_41_log__, alpha_40, tau_squared_40_log__, alpha_38, tau_squared_38_log__, alpha_37, tau_squared_37_log__, alpha_35, tau_squared_35_log__, alpha_33, tau_squared_33_log__, alpha_27, tau_squared_27_log__, alpha_24, tau_squared_24_log__, alpha_23, tau_squared_23_log__, alpha_21, tau_squared_21_log__, alpha_18, tau_squared_18_log__, alpha_17, tau_squared_17_log__, alpha_15, tau_squared_15_log__, alpha_14, tau_squared_14_log__, alpha_12, tau_squared_12_log__, alpha_9, tau_squared_9_log__, alpha_8, tau_squared_8_log__, alpha_5, tau_squared_5_log__, alpha_1, tau_squared_1_log__]\n",
      "100%|██████████| 2000/2000 [00:24<00:00, 82.91it/s]\n"
     ]
    }
   ],
   "source": [
    "a_x_prediction = []\n",
    "t_x_prediction = []\n",
    "# Sample the a_x, t_x latent variables for the prediction tweets\n",
    "with pm.Model() as time_prediction:\n",
    "    i_prime = 0\n",
    "    for i in range(num_root_tweets):\n",
    "        if not partition_assignment[i]:\n",
    "            t_x = pm.InverseGamma('tau_squared_{}'.format(i), alpha=hyperparams['a_tau'], beta=hyperparams['b_tau'])\n",
    "            a_x = pm.Normal('alpha_{}'.format(i), mu=hyperparams['alpha'], tau=1/hyperparams['sigma_squared_delta'])        \n",
    "            l_x = pm.Normal('log_{}'.format(i), mu=a_x, sd=t_x**0.5, observed=log_s_j_x[i_prime])\n",
    "            i_prime += 1\n",
    "        \n",
    "# Run and fit our model\n",
    "with time_prediction:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)\n",
    "    for i in range(num_root_tweets):\n",
    "        if not partition_assignment[i]:\n",
    "            a_x_prediction.append(np.mean(trace['alpha_{}'.format(i)]))\n",
    "            t_x_prediction.append(np.mean(trace['tau_squared_{}'.format(i)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retweet(pm.Continuous):\n",
    "    def __init__(self, alpha_x, t_x, S_x, m_t, t, T, *args, **kwargs):\n",
    "        super(Retweet, self).__init__(*args, **kwargs)\n",
    "        self.alpha_x = alpha_x\n",
    "        self.t_x = t_x\n",
    "        self.S_x = S_x\n",
    "        self.m_t = m_t\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "    \n",
    "    def logp(self, M):\n",
    "        nCr = lambda x,y: math.factorial(x) / (math.factorial(y) * math.factorial(x - y))\n",
    "        choose_func = theano.function([a,b], nCr(a,b))\n",
    "        choose_term = tt.prod(choose_func(M, self.m_t))\n",
    "\n",
    "        gauss = scipy.stats.norm(self.alpha_x, self.t_x)\n",
    "        F = lambda x: gauss.cdf(x)\n",
    "        f_func = theano.function([a,b], lambda x,y: (1 - F(x))**y)\n",
    "        f_term = tt.prod(f_func(tt.log(self.t - self.T), M - self.m_t))\n",
    "\n",
    "\n",
    "        rxn_term = tt.prod(gauss.pdf(self.S_x))\n",
    "\n",
    "        return math.log(rxn_term * f_term * choose_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retweet(pm.Discrete):\n",
    "    def __init__(self, alpha_x, t_x, S_x, m_t, t, b_x, f_x, *args, **kwargs):\n",
    "        super(Retweet, self).__init__(*args, **kwargs)\n",
    "        self.alpha_x = alpha_x\n",
    "        self.t_x = t_x\n",
    "        self.S_x = S_x\n",
    "        self.m_t = m_t\n",
    "        self.t = t\n",
    "        self.b_x = b_x\n",
    "        self.f_x = f_x\n",
    "        self.i = i\n",
    "    \n",
    "    def logp(self, M):\n",
    "        if M < self.m_t:\n",
    "            return -np.inf\n",
    "        \n",
    "        nCr = lambda x,y: math.factorial(x) / (math.factorial(y) * math.factorial(x - y))\n",
    "        gauss = scipy.stats.norm(self.alpha_x, self.t_x)\n",
    "        F = lambda x: gauss.cdf(x)\n",
    "        \n",
    "        choose_term_1 = nCr(M, self.m_t)       \n",
    "        f_term = tt.prod(f_func(tt.log(self.t - self.S_x), M - self.m_t))\n",
    "        choose_term_2 = nCr(self.f_x, M)\n",
    "        binomial_term = (self.b ** M) * (1 - b) ** (f_x - M)\n",
    "\n",
    "        return math.log(choose_term_1 * f_term * choose_term_2 * binomial_term) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 7.3624630711436341,\n",
       " 'sigma_squared_delta': 0.58742005712857215,\n",
       " 'a_tau': 5.5645302260927982,\n",
       " 'b_tau': 19.311009905771179,\n",
       " 'sigma_squared_b': 5.1719876665358724,\n",
       " 'beta_0': -7.9057724341651188,\n",
       " 'beta_F': -0.089856350130540039,\n",
       " 'beta_d': -6.2727785270466097}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraped from one run\n",
    "hyperparams = {}\n",
    "hyperparams['alpha'] = 7.3624630711436341\n",
    "hyperparams['sigma_squared_delta'] = 0.58742005712857215\n",
    "hyperparams['a_tau'] = 5.5645302260927982\n",
    "hyperparams['b_tau'] = 19.311009905771179\n",
    "hyperparams['sigma_squared_b'] = 5.1719876665358724\n",
    "hyperparams['beta_0'] = -7.9057724341651188\n",
    "hyperparams['beta_F'] = -0.089856350130540039\n",
    "hyperparams['beta_d'] = -6.2727785270466097\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Variables do not support boolean operations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-62ffc642b11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlogit_b_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logit_b_j'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sigma_squared_b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mb_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_b_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'likelihood_{}_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_x_prediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_x_prediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph_prediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/distributions/distribution.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtotal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name needs to be a string but got: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/model.py\u001b[0m in \u001b[0;36mVar\u001b[0;34m(self, name, dist, data, total_size)\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m                     var = FreeRV(name=name, distribution=dist,\n\u001b[0;32m--> 752\u001b[0;31m                                  total_size=total_size, model=self)\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_RVs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pymc3/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, type, owner, index, name, distribution, total_size, model)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             self.tag.test_value = np.ones(\n\u001b[1;32m   1129\u001b[0m                 distribution.shape, distribution.dtype) * distribution.default()\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogp_elemwiset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;31m# The logp might need scaling in minibatches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0;31m# This is done in `Factor`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-d12cf9c1fbb9>\u001b[0m in \u001b[0;36mlogp\u001b[0;34m(self, M)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/theano/tensor/var.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             raise TypeError(\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0;34m\"Variables do not support boolean operations.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             )\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Variables do not support boolean operations."
     ]
    }
   ],
   "source": [
    "with pm.Model() as graph_prediction:\n",
    "    for i in range(len(T)):\n",
    "        # TODO: Generate S_x, m_t, t for each prediction tweet\n",
    "        for j in range(len(T[i])):\n",
    "            u_j = hyperparams['beta_0'] + hyperparams['beta_F'] * pm.math.log(f_x[i][j]+1) + beta_d * pm.math.log(d[i][j]+1)\n",
    "            logit_b_j = pm.Normal('logit_b_j', mu=u_j, tau=1/hyperparams['sigma_squared_b'], shape=len(depth))\n",
    "            b_j = pm.math.invlogit(logit_b_j)\n",
    "            likelihood = Retweet('likelihood_{}_{}'.format(i,j), alpha_x = a_x_prediction[i], t_x = t_x_prediction[i], S_x = S_x[i][j], m_t = m_t[i][j], t = t[i], b_x=b_j, f_x = f_x[i][j], testval=m_t[i][j])\n",
    "        \n",
    "with graph_prediction:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
