{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import scipy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xlrd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../RetweetDataAOAS/retweet_data/'\n",
    "root_tweet_names = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "num_root_tweets = len(root_tweet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_name_to_index = {}\n",
    "for i in range(num_root_tweets):\n",
    "    tweet_name_to_index[root_tweet_names[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_partition_file_name(name):\n",
    "    root = name.split('.')\n",
    "    items = root[0].split('_')\n",
    "    items[-2], items[-1] = items[-1], items[-2]\n",
    "    return \".\".join([\"_\".join(items), root[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_path = '../Partition_1.xlsx'\n",
    "partition = pd.read_excel(partition_path)\n",
    "partition_assignments = {}\n",
    "for index, row in partition.iterrows():\n",
    "    training_file_name = format_partition_file_name(row['Training'])\n",
    "    prediction_file_name = format_partition_file_name(row['Prediction'])\n",
    "    partition_assignments[tweet_name_to_index[training_file_name]] = True\n",
    "    partition_assignments[tweet_name_to_index[prediction_file_name]] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces a dictionary of dataframes for each tweetfile, with initial \n",
    "# preprocessing\n",
    "observation_probability = .25\n",
    "fields = ['RetweetCount', 'UserId', 'ScreenName', 'FollowerCount', \n",
    "          'DistanceFromRoot','Time', 'ParentScreenName', 'Text']\n",
    "tweet_dfs = []\n",
    "for i in range(num_root_tweets):\n",
    "    tweet_df = pd.read_csv(path+root_tweet_names[i], sep=\"\\t\", header=None, \n",
    "                         quoting=csv.QUOTE_NONE, names=fields, encoding = \"ISO-8859-1\")\n",
    "    if not partition_assignments[i]:\n",
    "        tweet_df = tweet_df.head(int(observation_probability * tweet_df.shape[0]))\n",
    "    tweet_df['Time'] = pd.to_datetime(tweet_df['Time'])\n",
    "\n",
    "    screen_name_index = {}\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        screen_name_index[row['ScreenName']] = index\n",
    "    tweet_df['ParentDfIndex'] = tweet_df['ParentScreenName'].map(screen_name_index)\n",
    "    tweet_df.fillna(0)\n",
    "    tweet_dfs.append(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_deltas(tweet_df):\n",
    "    t_x = tweet_df.tail(1)['Time']\n",
    "    time_deltas = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        time_delta = t_x - row['Time']\n",
    "        time_deltas.append(time_delta.seconds)\n",
    "    return time_deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary of reaction times S_j^x keyed by user id\n",
    "def generate_reaction_times(tweet_df):\n",
    "    reaction_times = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if index > 0:\n",
    "            reaction_time = row['Time'] - tweet_df.at[row['ParentDfIndex'],\n",
    "                                                      'Time']\n",
    "            reaction_times.append(reaction_time)\n",
    "    return reaction_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary of M_j^x keyed by user id\n",
    "def generate_number_of_follower_who_retweet(tweet_df):\n",
    "    number_of_follower_who_retweet = {}\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if row['UserId'] not in number_of_follower_who_retweet:\n",
    "            number_of_follower_who_retweet[row['UserId']] = 0\n",
    "        parent_user_id = tweet_df.at[row['ParentDfIndex'], 'UserId']\n",
    "        number_of_follower_who_retweet[parent_user_id] += 1\n",
    "    return number_of_follower_who_retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_info(tweet_df, rt_dic):\n",
    "    depth = []\n",
    "    num_followers = []\n",
    "    num_retweets = []\n",
    "    for index, row in tweet_df.iterrows():\n",
    "        if row['UserId'] != 'None' and row['DistanceFromRoot'] < 1:\n",
    "            depth.append(row['DistanceFromRoot'])\n",
    "            if row['FollowerCount'] == 'None':\n",
    "                num_followers.append(0)\n",
    "            else:\n",
    "                num_followers.append(int(row['FollowerCount']))\n",
    "            if row['UserId'] in rt_dic:\n",
    "                num_retweets.append(rt_dic[row['UserId']])\n",
    "            else:\n",
    "                num_reweets.append(0)\n",
    "    return depth, num_followers, num_retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {}\n",
    "log_s_j_x = []\n",
    "# These three are parallel arrays for each retweet \n",
    "# (depth[i], num_followers[i], num_retweets[i] all refer to the same retweet)\n",
    "depth = []\n",
    "num_followers = []\n",
    "num_retweets = []\n",
    "\n",
    "# Prediction arrays\n",
    "t = []\n",
    "T = []\n",
    "S_x = []\n",
    "m_t = []\n",
    "for i in range(len(root_tweet_names)):\n",
    "    if partition_assignment[i]:\n",
    "        s_j_x = generate_reaction_times(tweet_dfs[i])\n",
    "        log_s_j_x.append([np.log(i.seconds) for i in s_j_x])\n",
    "        M_j_dic = generate_number_of_follower_who_retweet(tweet_dfs[i])\n",
    "        d_x, M_j_x, m_j_x = generate_graph_info(tweet_dfs[i], M_j_dic)\n",
    "        depth.extend(d_x)\n",
    "        num_followers.extend(M_j_x)\n",
    "        num_retweets.extend(m_j_x)\n",
    "    else:\n",
    "        t.append(tweet_dfs[i].tail(1)['Time'])\n",
    "        T.append(generate_time_deltas(tweet_dfs[i]))\n",
    "        S_x.append(generate_reaction_times(tweet_dfs[i]))\n",
    "        M_j_dic = generate_number_of_follower_who_retweet(tweet_dfs[i])\n",
    "        d_x, M_j_x, m_j_x = generate_graph_info(tweet_dfs[i], M_j_dic)\n",
    "        m_t.append(m_j_x)\n",
    "depth = np.array(depth)\n",
    "num_followers = np.array(num_followers)\n",
    "num_retweets = np.array(num_retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the time-related hyperparameters\n",
    "with pm.Model() as time_model:\n",
    "    # global model parameters\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=100)\n",
    "    sigma_squared_delta = pm.InverseGamma('sigma_squared_delta', alpha=2, beta=2)\n",
    "    log_a_tau = pm.Normal('log_a_tau', mu=0, sd=10)\n",
    "    b_tau = pm.Gamma('b_tau', alpha=1, beta=.002)\n",
    "    \n",
    "    # log-normal model for reaction times, nonrecursive...\n",
    "    a_tau = pm.Deterministic('a_tau', pm.math.exp(log_a_tau))\n",
    "    for i in range(num_root_tweets):\n",
    "        t_x = pm.InverseGamma('tau_x_squared_' + str(i), alpha=a_tau, beta=b_tau)\n",
    "        a_x = pm.Normal('alpha_x_' + str(i), mu=alpha, tau=1/sigma_squared_delta)        \n",
    "        l_x = pm.Normal('log_s_j_x_' + str(i), mu=a_x, sd=t_x**0.5, observed=log_s_j_x[i])\n",
    "        \n",
    "# Run and fit our model\n",
    "with time_model:\n",
    "    trace = pm.sample(1000, tune=2000, cores=4)\n",
    "    time_params = ['alpha', 'sigma_squared_delta', 'a_tau', 'b_tau']\n",
    "    # Extract the hyperparameters\n",
    "    for param in time_params:\n",
    "        hyperparams[param] = np.mean(trace[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the graph related hyperparameters\n",
    "with pm.Model() as graph_model:\n",
    "    sigma_squared_b = pm.InverseGamma('sigma_squared_b', alpha=0.5, beta=0.5, testval=10000)\n",
    "    beta_0 = pm.Normal('beta_0', mu=0, tau=1/10000, testval=1.99)\n",
    "    beta_F = pm.Normal('beta_F', mu=0, tau=1/10000, testval=-0.79)\n",
    "    beta_d = pm.Normal('beta_d', mu=0, tau=1/10000)\n",
    "    \n",
    "    u_j = beta_0 + beta_F * pm.math.log(num_followers+1) + beta_d * pm.math.log(depth+1)\n",
    "    logit_b_j = pm.Normal('logit_b_j', mu=u_j, tau=1/sigma_squared_b, shape=len(depth))\n",
    "    b_j = pm.math.invlogit(logit_b_j)\n",
    "    M_j = pm.Binomial('retweet_count M_j', n=num_followers, p=b_j, observed=num_retweets)\n",
    "    \n",
    "    \n",
    "# Run and fit our model\n",
    "with graph_model:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)\n",
    "    graph_params = ['sigma_squared_b', 'beta_0', 'beta_F', 'beta_d']\n",
    "    # Extract the hyperparameters\n",
    "    for param in graph_params:\n",
    "        hyperparams[param] = np.mean(trace[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph all of the important distributions for time hyper-paramers\n",
    "with time_model:\n",
    "    imp_dists = ['alpha', 'sigma_squared_delta', 'a_tau', 'b_tau']\n",
    "    fig, axs = plt.subplots(2,2, figsize = (8,8))\n",
    "    for i in range(len(imp_dists)):\n",
    "        ax1 = axs[int(i/2)][i%2]\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set(ylim=(-.25, .25))\n",
    "        var = imp_dists[i]\n",
    "        sns.distplot(trace[var], ax=ax1).set_title(var);\n",
    "        sns.boxplot(trace[var], ax=ax2, notch=True)\n",
    "        print(var + \": \" + str(np.mean(trace[var])) + \"(\" + str(np.std(trace[var])) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_squared_b: 0.906456624789(0.188863500713)\n",
      "beta_0: 2.14994659679(0.830771062302)\n",
      "beta_F: -0.789875139102(0.0624924455684)\n",
      "beta_d: -0.225780593314(102.146961865)\n"
     ]
    }
   ],
   "source": [
    "# graph all of the important distributions for graph hyper-paramers\n",
    "with graph_model:\n",
    "    imp_dists = ['sigma_squared_b', 'beta_0', 'beta_F', 'beta_d']\n",
    "#    fig, axs = plt.subplots(2,2, figsize = (8,8))\n",
    "    for i in range(len(imp_dists)):\n",
    "#         ax1 = axs[int(i/2)][i%2]\n",
    "#         ax2 = ax1.twinx()\n",
    "#         ax2.set(ylim=(-.25, .25))\n",
    "        var = imp_dists[i]\n",
    "#         sns.distplot(trace[var], ax=ax1).set_title(var);\n",
    "#         sns.boxplot(trace[var], ax=ax2, notch=True)\n",
    "        print(var + \": \" + str(np.mean(trace[var])) + \"(\" + str(np.std(trace[var])) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_x_prediction = []\n",
    "t_x_prediction = []\n",
    "# Sample the a_x, t_x latent variables for the prediction tweets\n",
    "with pm.Model as time_prediction:\n",
    "    for i in range(len(prediction_tweets)):\n",
    "        t_x = pm.InverseGamma('tau_x_squared_' + str(i), alpha=hyperparams.a_tau, beta=hyperparams.b_tau)\n",
    "        a_x = pm.Normal('alpha_x_' + str(i), mu=hyperparams.alpha, tau=1/hyperparams.sigma_squared_delta)        \n",
    "        l_x = pm.Normal('log_s_j_x_' + str(i), mu=a_x, sd=t_x**0.5, observed=log_s_j_x[i])\n",
    "        \n",
    "# Run and fit our model\n",
    "with time_prediction:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)\n",
    "    for i in range(len(prediction_tweets)):\n",
    "        a_x_prediction.append(np.mean(trace['alpha_x_' + str(i)]))\n",
    "        t_x_prediction.append(np.mean(trace['tau_x_squared_' + str(i)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retweet(pm.Continuous):\n",
    "    def __init__(a_x, t_x, S_x, m_t, t, T, *args, **kwargs):\n",
    "        super(Retweet, self).__init__(*args, **kwargs)\n",
    "        self.a_x = a_x\n",
    "        self.t_x = t_x\n",
    "        self.S_x = S_x\n",
    "        self.m_t = m_t\n",
    "        self.t = t\n",
    "        self.T = T\n",
    "    \n",
    "    def logp(self, M):\n",
    "        nCr = lambda x,y: math.factorial(x) / (math.factorial(y) * math.factorial(x - y))\n",
    "        choose_func = theano.function([a,b] nCr(a,b))\n",
    "        choose_term = tt.prod(choose_func(M, self.m_t))\n",
    "\n",
    "        gauss = scipy.stats.norm(self.a_x, self.t_x)\n",
    "        F = lambda x: gauss.cdf(x)\n",
    "        f_func = theano.function([a,b], lambda x,y: (1 - F(x))**y)\n",
    "        f_term = tt.prod(f_func(tt.log(self.t - self.T), M - self.m_t))\n",
    "\n",
    "\n",
    "        rxn_term = tt.prod(gauss.pdf(self.S_x))\n",
    "\n",
    "        return math.log(rxn_term * f_term * choose_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as graph_prediction:\n",
    "    for i in range(len(prediction_tweets)):\n",
    "        # TODO: Generate S_x, m_t, t for each prediction tweet\n",
    "        likelihood = Retweet(a_x = a_x_prediction[i], t_x = t_x_prection[i], S_x = S_x[i], m_t = m_t[i], t = t[i], T = T[i])\n",
    "        \n",
    "with graph_prediction:\n",
    "    trace = pm.sample(1000, tune=1000, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
